# -*- coding: utf-8 -*-
"""py_metrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vr7kDgTFG7ieLXrfN0RSI0eEi2vx1feg
"""

import math

"""#NDCG"""

def get_logs(scores):
  return [scores[i]/math.log2(i+2) for i in range(len(scores))]

def ndcg_score(scores, true_scores):
  
  dcg = sum(get_logs(scores))
  idcg = sum(get_logs(true_scores))

  if idcg == 0:
    idcg = 1000000

  ndcg = dcg/idcg

  return ndcg

"""# Top N Accuracy"""

def topNacc(preds, n, trues):

  got_right = []
  for pred in preds[:n]:
    if pred in trues:
      got_right.append(pred)
  
  acc = len(got_right)/n
  return acc

def acc_scores(all_preds, n, all_trues):

  scores = []
  for query_preds, true in zip(all_preds, all_trues):
    scores.append(topNacc(query_preds, n, true))
  
  return scores

"""# Mean Reciprocal Rank"""

def get_rr_score(query):
  preds, trues = query[0], query[1]
  if len(preds) == 0:
    return 0
  for i in range(len(preds)):
    if preds[i] in trues:
      return 1/(i+1)
    if i == (len(preds)-1):
      return 0

def mean_rr(queries):
  return (sum([get_rr_score(query) for query in queries]))/len(queries)